{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on toxic comments\n",
    "# Building Transformer from scratch using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   #For data frames, reading data, data processing, and analysis\n",
    "import numpy as np   #For numerical computations\n",
    "import string   # For string operations and constants.\n",
    "import re   # Used for regular expression matching and operations.\n",
    "from nltk.corpus import stopwords   # Import a list of common stopwords\n",
    "import contractions     # For expand abbreviations\n",
    "import nltk   # Import Natural language Toolkit\n",
    "from nltk.tokenize import word_tokenize   # For tokenization\n",
    "from nltk.stem import WordNetLemmatizer     # For lemmatization\n",
    "import torch    # Torch library\n",
    "import torch.nn as nn       # importing various classes and functions dor building neural networks\n",
    "import torch.nn.functional as F     # provides various functions for performing operations\n",
    "from sklearn.model_selection import train_test_split     # For splitting data into train and test\n",
    "from torch.utils.data import TensorDataset,DataLoader  # Dataset: create dataset from tensors, DataLoader: load data from dataset\n",
    "import torch.optim as optim     # Provides optimazation algorithms\n",
    "from sklearn.metrics import classification_report     # For showing classfication metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK tokenizer data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK Stop Words data\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "data = pd.read_csv('C:\\\\Users\\\\LENOVO\\\\Desktop\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display Data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display each column with its number of nulls and data types\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are no nulls\n",
    "- All are int columns except columns [id, comment_text] which are object types\n",
    "- Total 159571 rows and 8 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805  \n",
       "std         0.216627       0.093420  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display int columns and their counts, mean, standard deviation, minimum, maximum,and three quantiles\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has 159571 rows and 8 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "comment_text     object\n",
       "toxic             int64\n",
       "severe_toxic      int64\n",
       "obscene           int64\n",
       "threat            int64\n",
       "insult            int64\n",
       "identity_hate     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all data are int except [id] and [comment_text] which are object data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nulls data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nulls in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               159571\n",
       "comment_text     159571\n",
       "toxic                 2\n",
       "severe_toxic          2\n",
       "obscene               2\n",
       "threat                2\n",
       "insult                2\n",
       "identity_hate         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of unique values in each column\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data values are unique as each id and its comment is related to only one person and the other columns are binary (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated comments =  0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the count of duplicated comments in the 'comment_text' column\n",
    "duplicated_count = data['comment_text'].duplicated().sum()\n",
    "print(\"Number of duplicated comments = \", duplicated_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    144277\n",
       "1     15294\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of values in column [toxic]\n",
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 144277 people didn't write toxic comments, while the other 15294 people wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "severe_toxic\n",
       "0    157976\n",
       "1      1595\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of values in column [severe toxic]\n",
    "data['severe_toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "157976 people didn't write severe toxic comments, while the other 1595 people wrote \n",
    "--> It seems to be biased towards class 0 so its unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obscene\n",
       "0    151122\n",
       "1      8449\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of values in column [obscene]\n",
    "data['obscene'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "151122 people didn't write obscene comments but the other 8449 people wrote --> It seems to be biased towards class 0 so its unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threat\n",
       "0    159093\n",
       "1       478\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of values in column [threat]\n",
    "data['threat'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "159093 people didn't write threat comments but the other 478 people wrote\n",
    "--> It is highly biased towards class 0 so its unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "insult\n",
       "0    151694\n",
       "1      7877\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of values in column [insult]\n",
    "data['insult'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "151694 people didn't write insult comments but the other 7877 wrote\n",
    "--> It seems to be biased towards class 0 so its unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "identity_hate\n",
       "0    158166\n",
       "1      1405\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display number of values in column [identity hate]\n",
    "data['identity_hate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "158166 people didn't write identity hate comments but the other 1405 people wrote\n",
    "--> It seems to be biased towards class 0 so its unbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above analsysis made with value counts for each binary column, found thet column toxic is the most balanced column compared to the others\n",
    "- Also, found that number of (zeros) for each column is greater than number of (ones)\n",
    "- So for reassuring this, i will split the data in two classes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into appropriate and inappropriate comments for more exploring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the data which has appropriate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143341</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143342</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143343</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143344</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143345</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143346 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "143341  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "143342  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "143343  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "143344  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "143345  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "143341      0             0        0       0       0              0  \n",
       "143342      0             0        0       0       0              0  \n",
       "143343      0             0        0       0       0              0  \n",
       "143344      0             0        0       0       0              0  \n",
       "143345      0             0        0       0       0              0  \n",
       "\n",
       "[143346 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data which has no 1 in any of the binary columns as the (zerosData)\n",
    "zerosData = (data[\"toxic\"]==0)&(data[\"severe_toxic\"]==0)&(data[\"obscene\"]==0)&(data[\"threat\"]==0)&(data[\"insult\"]==0)&(data[\"identity_hate\"]==0)\n",
    "\n",
    "# Put it in variable (Appropriate_comments)\n",
    "Appropriate_comments = data[zerosData]\n",
    "\n",
    "# Reset the index of the 'Appropriate_comments'\n",
    "Appropriate_comments.reset_index(drop=True, inplace=True)\n",
    "Appropriate_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the data which has inappropriate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16220</th>\n",
       "      <td>fef4cf7ba0012866</td>\n",
       "      <td>\"\\n\\n our previous conversation \\n\\nyou fuckin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16221</th>\n",
       "      <td>ff39a2895fc3b40e</td>\n",
       "      <td>YOU ARE A MISCHIEVIOUS PUBIC HAIR</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16222</th>\n",
       "      <td>ffa33d3122b599d6</td>\n",
       "      <td>Your absurd edits \\n\\nYour absurd edits on gre...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16223</th>\n",
       "      <td>ffb47123b2d82762</td>\n",
       "      <td>\"\\n\\nHey listen don't you ever!!!! Delete my e...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16224</th>\n",
       "      <td>ffbdbb0483ed0841</td>\n",
       "      <td>and i'm going to keep posting the stuff u dele...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16225 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "0      0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "1      0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "2      0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "3      001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "4      00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "...                 ...                                                ...   \n",
       "16220  fef4cf7ba0012866  \"\\n\\n our previous conversation \\n\\nyou fuckin...   \n",
       "16221  ff39a2895fc3b40e                  YOU ARE A MISCHIEVIOUS PUBIC HAIR   \n",
       "16222  ffa33d3122b599d6  Your absurd edits \\n\\nYour absurd edits on gre...   \n",
       "16223  ffb47123b2d82762  \"\\n\\nHey listen don't you ever!!!! Delete my e...   \n",
       "16224  ffbdbb0483ed0841  and i'm going to keep posting the stuff u dele...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0          1             1        1       0       1              0  \n",
       "1          1             0        0       0       0              0  \n",
       "2          1             0        0       0       0              0  \n",
       "3          1             0        1       0       1              1  \n",
       "4          1             0        1       0       1              0  \n",
       "...      ...           ...      ...     ...     ...            ...  \n",
       "16220      1             0        1       0       1              1  \n",
       "16221      1             0        0       0       1              0  \n",
       "16222      1             0        1       0       1              0  \n",
       "16223      1             0        0       0       1              0  \n",
       "16224      1             0        1       0       1              0  \n",
       "\n",
       "[16225 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display data which has 1 in one or more of the binary columns as the (onesData)\n",
    "onesData = (data[\"toxic\"]==1)|(data[\"severe_toxic\"]==1)|(data[\"obscene\"]==1)|(data[\"threat\"]==1)|(data[\"insult\"]==1)|(data[\"identity_hate\"]==1)\n",
    "\n",
    "# Put it in variable (Inappropriate_comments)\n",
    "Inappropriate_comments = data[onesData]\n",
    "\n",
    "# Reset the index of the 'Inappropriate_comments'\n",
    "Inappropriate_comments.reset_index(drop=True, inplace=True)\n",
    "Inappropriate_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the percentage of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of appropriate data is:  89.83211235124176\n",
      "Percentage of inappropriate data is:  10.167887648758233\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows from both dataframes\n",
    "num_appropriate = data[zerosData].shape[0]\n",
    "num_inappropriate = data[onesData].shape[0]\n",
    "\n",
    "# divide the number by whole length of data * 100 to get percentage\n",
    "Percent_appropriate_data = (num_appropriate / len(data))*100\n",
    "Percent_inappropriate_data = (num_inappropriate / len(data))*100\n",
    "\n",
    "print(\"Percentage of appropriate data is: \", Percent_appropriate_data)\n",
    "print(\"Percentage of inappropriate data is: \", Percent_inappropriate_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- approximatly 90% of comments are appropriate comments while the othe 10% are inappropriate comments\n",
    "- so the data is imbalanced as it's highly biased towards class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to deal with:\n",
    "- URLs --> by removing them\n",
    "- Abbreviations --> will expand them\n",
    "- Special characters --> by removing them\n",
    "- Numeric and Punctuation characters --> by removing them\n",
    "- Stop words --> by removing them\n",
    "- Remove duplicates after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pattern of URL\n",
    "url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "\n",
    "# Define a pattern of special characters\n",
    "pattern_spec_char = r'[^A-Za-z0-9\\s]+'\n",
    "\n",
    "# Define a pattern of numeric and punctuation characters\n",
    "numeric_punctuation_pattern = r'[\\d' + string.punctuation + '\\n]+'\n",
    "\n",
    "# Collect all stop words and save it to variable (stop_words)\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to count numbers of things i want to clean it from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers(data , column):\n",
    "\n",
    "    # Count rows with URLs in the 'comment_text' column\n",
    "    print(\"Number of rows with URLs now = \", data['comment_text'].str.contains(url_pattern, case=False, regex=True).sum())\n",
    "\n",
    "    # Count rows with special characters in the 'comment_text' column\n",
    "    print(\"Number of rows with special characters now = \", data[column].str.contains(pattern_spec_char).sum())\n",
    "\n",
    "    # Count rows with numeric and punctiuations in the 'comment_text' column\n",
    "    print(\"Number of rows with numeric and punctiuation characters now = \", data[column].str.contains(numeric_punctuation_pattern).sum())\n",
    "    \n",
    "    # Count rows with stop words in the 'comment_text' column\n",
    "    print(\"Number of rows with stop words now = \", sum(any(token in stop_words for token in nltk.word_tokenize(column)) for column in data[column]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data, column):\n",
    "\n",
    "    # Convert text to lower case\n",
    "    data[column] = data[column].str.lower()\n",
    "\n",
    "    # Remove the url by replacing it with empty string\n",
    "    data[column] = data[column].str.replace(url_pattern, '', case=False, regex=True)\n",
    "\n",
    "    # Apply function Expand contractions\n",
    "    data[column] = data[column].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "    # Remove special characters \n",
    "    data[column] = data[column].apply(lambda x: re.sub(pattern_spec_char, '', x))\n",
    "\n",
    "    # Remove the numeric and punctiuational characters \n",
    "    data[column] = data[column].apply(lambda x: re.sub(numeric_punctuation_pattern, ' ', x))\n",
    " \n",
    "    # Tokenizing the text first, then remove stop words using list fo stop words provide above\n",
    "    # Then join the cleaned text back into a string\n",
    "    data[column] = data[column].apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with URLs now =  5114\n",
      "Number of rows with special characters now =  155146\n",
      "Number of rows with numeric and punctiuation characters now =  156447\n",
      "Number of rows with stop words now =  152520\n"
     ]
    }
   ],
   "source": [
    "check_numbers(data,'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data(data,'comment_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with URLs now =  0\n",
      "Number of rows with special characters now =  0\n",
      "Number of rows with numeric and punctiuation characters now =  0\n",
      "Number of rows with stop words now =  0\n"
     ]
    }
   ],
   "source": [
    "# To check if things removed successfully\n",
    "check_numbers(data,'comment_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates after cleaning and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated comments after cleaning =  1897\n"
     ]
    }
   ],
   "source": [
    "# Calculate the count of duplicated comments in the 'comment_text' column\n",
    "duplicated_count = data['comment_text'].duplicated().sum()\n",
    "print(\"Number of duplicated comments after cleaning = \", duplicated_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157674 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[157674 rows x 8 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the duplicates\n",
    "data.drop_duplicates(subset='comment_text', inplace=True)\n",
    "\n",
    "# Reset index\n",
    "data.reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_count = data['comment_text'].duplicated().sum()\n",
    "duplicated_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Data now is cleaned, free from duplicates, URLs, stop words, and special, numeric, and punctuation characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data using function word_tokenize\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157674 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "159566      0             0        0       0       0              0   \n",
       "159567      0             0        0       0       0              0   \n",
       "159568      0             0        0       0       0              0   \n",
       "159569      0             0        0       0       0              0   \n",
       "159570      0             0        0       0       0              0   \n",
       "\n",
       "                                        tokenized_comment  \n",
       "0       [explanation, edits, made, username, hardcore,...  \n",
       "1       [daww, matches, background, colour, seemingly,...  \n",
       "2       [hey, man, really, trying, edit, war, guy, con...  \n",
       "3       [make, real, suggestions, improvement, wondere...  \n",
       "4                     [sir, hero, chance, remember, page]  \n",
       "...                                                   ...  \n",
       "159566  [second, time, asking, view, completely, contr...  \n",
       "159567        [ashamed, horrible, thing, put, talk, page]  \n",
       "159568  [spitzer, umm, actual, article, prostitution, ...  \n",
       "159569  [looks, like, actually, put, speedy, first, ve...  \n",
       "159570  [really, think, understand, came, idea, bad, r...  \n",
       "\n",
       "[157674 rows x 9 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new column in the data that have the tokenized comment\n",
    "\n",
    "# Apply tokenize_text to the original 'comment_text'\n",
    "data['tokenized_comment'] = data['comment_text'].apply(tokenize_text)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column appeard with list of tokenized words for each row in [comment_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize tokenized comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lemmatization is the process of reducing words to their base form (lemma) \n",
    "- Lemmatization preserves the semantic meaning of words which we must consider in further analysis and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize a list of words\n",
    "def lemmatize_words(tokenized_comment):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokenized_comment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157674 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "159566      0             0        0       0       0              0   \n",
       "159567      0             0        0       0       0              0   \n",
       "159568      0             0        0       0       0              0   \n",
       "159569      0             0        0       0       0              0   \n",
       "159570      0             0        0       0       0              0   \n",
       "\n",
       "                                        tokenized_comment  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "159566  [second, time, asking, view, completely, contr...   \n",
       "159567        [ashamed, horrible, thing, put, talk, page]   \n",
       "159568  [spitzer, umm, actual, article, prostitution, ...   \n",
       "159569  [looks, like, actually, put, speedy, first, ve...   \n",
       "159570  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                       lemmatized_comment  \n",
       "0       explanation edits made username hardcore metal...  \n",
       "1       daww match background colour seemingly stuck t...  \n",
       "2       hey man really trying edit war guy constantly ...  \n",
       "3       make real suggestion improvement wondered sect...  \n",
       "4                           sir hero chance remember page  \n",
       "...                                                   ...  \n",
       "159566  second time asking view completely contradicts...  \n",
       "159567               ashamed horrible thing put talk page  \n",
       "159568  spitzer umm actual article prostitution ring c...  \n",
       "159569  look like actually put speedy first version de...  \n",
       "159570  really think understand came idea bad right aw...  \n",
       "\n",
       "[157674 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply lemmatization to the 'tokenized_comment' column\n",
    "data['lemmatized_comment'] = (data['tokenized_comment'].apply(lemmatize_words)).apply(lambda tokens: ' '.join(tokens))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column appered with the lemmatized version of the tokenized comment column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates after lemmatization and drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated comments after lemmatization =  37\n"
     ]
    }
   ],
   "source": [
    "# Calculate the count of duplicated comments in the 'lemmatized_comment' column\n",
    "duplicated_count = data['lemmatized_comment'].duplicated().sum()\n",
    "print(\"Number of duplicated comments after lemmatization = \", duplicated_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157637 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "159566      0             0        0       0       0              0   \n",
       "159567      0             0        0       0       0              0   \n",
       "159568      0             0        0       0       0              0   \n",
       "159569      0             0        0       0       0              0   \n",
       "159570      0             0        0       0       0              0   \n",
       "\n",
       "                                        tokenized_comment  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "159566  [second, time, asking, view, completely, contr...   \n",
       "159567        [ashamed, horrible, thing, put, talk, page]   \n",
       "159568  [spitzer, umm, actual, article, prostitution, ...   \n",
       "159569  [looks, like, actually, put, speedy, first, ve...   \n",
       "159570  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                       lemmatized_comment  \n",
       "0       explanation edits made username hardcore metal...  \n",
       "1       daww match background colour seemingly stuck t...  \n",
       "2       hey man really trying edit war guy constantly ...  \n",
       "3       make real suggestion improvement wondered sect...  \n",
       "4                           sir hero chance remember page  \n",
       "...                                                   ...  \n",
       "159566  second time asking view completely contradicts...  \n",
       "159567               ashamed horrible thing put talk page  \n",
       "159568  spitzer umm actual article prostitution ring c...  \n",
       "159569  look like actually put speedy first version de...  \n",
       "159570  really think understand came idea bad right aw...  \n",
       "\n",
       "[157637 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "data.drop_duplicates(subset='lemmatized_comment', inplace=True)\n",
    "data.reset_index(drop = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary goal of creating a vocabulary is to map words in column ['comment_text'] to unique numerical indices to allow the model to work with numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data, column):\n",
    "    \n",
    "    # Create a dictionary that maps words to their indices and make '<PAD>' mapped to 0\n",
    "    word_to_index = {'<PAD>': 0}\n",
    "    \n",
    "    # Assign word in the order they appear in the data\n",
    "    idx = 1\n",
    "\n",
    "    # Iterate through each comment in the column\n",
    "    for comment in data[column]:\n",
    "\n",
    "        # Split the comment into words\n",
    "        words = comment.split()\n",
    "\n",
    "        # Iterate through each word in the comment\n",
    "        for word in words:\n",
    "\n",
    "            # If the word is not found in (word_to_index), it will be added with a unique index\n",
    "            if word not in word_to_index:\n",
    "                word_to_index[word] = idx\n",
    "\n",
    "                # After the word is added, the index increase by 1 to have a new word \n",
    "                idx += 1\n",
    "    \n",
    "    # Create a dictionary that maps indices back to words\n",
    "    index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "    \n",
    "    return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size =  204909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'explanation': 1,\n",
       " 'edits': 2,\n",
       " 'made': 3,\n",
       " 'username': 4,\n",
       " 'hardcore': 5,\n",
       " 'metallica': 6,\n",
       " 'fan': 7,\n",
       " 'reverted': 8,\n",
       " 'vandalism': 9,\n",
       " 'closure': 10,\n",
       " 'gas': 11,\n",
       " 'voted': 12,\n",
       " 'new': 13,\n",
       " 'york': 14,\n",
       " 'doll': 15,\n",
       " 'fac': 16,\n",
       " 'please': 17,\n",
       " 'remove': 18,\n",
       " 'template': 19,\n",
       " 'talk': 20,\n",
       " 'page': 21,\n",
       " 'since': 22,\n",
       " 'retired': 23,\n",
       " 'daww': 24,\n",
       " 'match': 25,\n",
       " 'background': 26,\n",
       " 'colour': 27,\n",
       " 'seemingly': 28,\n",
       " 'stuck': 29,\n",
       " 'thanks': 30,\n",
       " 'january': 31,\n",
       " 'utc': 32,\n",
       " 'hey': 33,\n",
       " 'man': 34,\n",
       " 'really': 35,\n",
       " 'trying': 36,\n",
       " 'edit': 37,\n",
       " 'war': 38,\n",
       " 'guy': 39,\n",
       " 'constantly': 40,\n",
       " 'removing': 41,\n",
       " 'relevant': 42,\n",
       " 'information': 43,\n",
       " 'talking': 44,\n",
       " 'instead': 45,\n",
       " 'seems': 46,\n",
       " 'care': 47,\n",
       " 'formatting': 48,\n",
       " 'actual': 49,\n",
       " 'info': 50,\n",
       " 'make': 51,\n",
       " 'real': 52,\n",
       " 'suggestion': 53,\n",
       " 'improvement': 54,\n",
       " 'wondered': 55,\n",
       " 'section': 56,\n",
       " 'statistic': 57,\n",
       " 'later': 58,\n",
       " 'subsection': 59,\n",
       " 'type': 60,\n",
       " 'accident': 61,\n",
       " 'think': 62,\n",
       " 'reference': 63,\n",
       " 'may': 64,\n",
       " 'need': 65,\n",
       " 'tidying': 66,\n",
       " 'exact': 67,\n",
       " 'format': 68,\n",
       " 'ie': 69,\n",
       " 'date': 70,\n",
       " 'etc': 71,\n",
       " 'noone': 72,\n",
       " 'else': 73,\n",
       " 'first': 74,\n",
       " 'preference': 75,\n",
       " 'style': 76,\n",
       " 'want': 77,\n",
       " 'let': 78,\n",
       " 'know': 79,\n",
       " 'appears': 80,\n",
       " 'backlog': 81,\n",
       " 'article': 82,\n",
       " 'review': 83,\n",
       " 'guess': 84,\n",
       " 'delay': 85,\n",
       " 'reviewer': 86,\n",
       " 'turn': 87,\n",
       " 'listed': 88,\n",
       " 'form': 89,\n",
       " 'eg': 90,\n",
       " 'wikipediagoodarticlenominationstransport': 91,\n",
       " 'sir': 92,\n",
       " 'hero': 93,\n",
       " 'chance': 94,\n",
       " 'remember': 95,\n",
       " 'congratulation': 96,\n",
       " 'well': 97,\n",
       " 'use': 98,\n",
       " 'tool': 99,\n",
       " 'cocksucker': 100,\n",
       " 'piss': 101,\n",
       " 'around': 102,\n",
       " 'work': 103,\n",
       " 'matt': 104,\n",
       " 'shirvington': 105,\n",
       " 'banned': 106,\n",
       " 'sorry': 107,\n",
       " 'word': 108,\n",
       " 'nonsense': 109,\n",
       " 'offensive': 110,\n",
       " 'anyway': 111,\n",
       " 'intending': 112,\n",
       " 'write': 113,\n",
       " 'anything': 114,\n",
       " 'articlewow': 115,\n",
       " 'would': 116,\n",
       " 'jump': 117,\n",
       " 'merely': 118,\n",
       " 'requesting': 119,\n",
       " 'encyclopedic': 120,\n",
       " 'one': 121,\n",
       " 'school': 122,\n",
       " 'selective': 123,\n",
       " 'breeding': 124,\n",
       " 'almost': 125,\n",
       " 'stub': 126,\n",
       " 'point': 127,\n",
       " 'animal': 128,\n",
       " 'short': 129,\n",
       " 'messy': 130,\n",
       " 'give': 131,\n",
       " 'must': 132,\n",
       " 'someone': 133,\n",
       " 'expertise': 134,\n",
       " 'eugenics': 135,\n",
       " 'alignment': 136,\n",
       " 'subject': 137,\n",
       " 'contrary': 138,\n",
       " 'dulithgow': 139,\n",
       " 'fair': 140,\n",
       " 'rationale': 141,\n",
       " 'imagewonjujpg': 142,\n",
       " 'uploading': 143,\n",
       " 'notice': 144,\n",
       " 'image': 145,\n",
       " 'specifies': 146,\n",
       " 'used': 147,\n",
       " 'wikipedia': 148,\n",
       " 'constitutes': 149,\n",
       " 'addition': 150,\n",
       " 'boilerplate': 151,\n",
       " 'also': 152,\n",
       " 'description': 153,\n",
       " 'specific': 154,\n",
       " 'using': 155,\n",
       " 'consistent': 156,\n",
       " 'go': 157,\n",
       " 'include': 158,\n",
       " 'uploaded': 159,\n",
       " 'medium': 160,\n",
       " 'consider': 161,\n",
       " 'checking': 162,\n",
       " 'specified': 163,\n",
       " 'find': 164,\n",
       " 'list': 165,\n",
       " 'edited': 166,\n",
       " 'clicking': 167,\n",
       " 'contribution': 168,\n",
       " 'link': 169,\n",
       " 'located': 170,\n",
       " 'top': 171,\n",
       " 'logged': 172,\n",
       " 'selecting': 173,\n",
       " 'dropdown': 174,\n",
       " 'box': 175,\n",
       " 'note': 176,\n",
       " 'lacking': 177,\n",
       " 'deleted': 178,\n",
       " 'week': 179,\n",
       " 'described': 180,\n",
       " 'criterion': 181,\n",
       " 'speedy': 182,\n",
       " 'deletion': 183,\n",
       " 'question': 184,\n",
       " 'ask': 185,\n",
       " 'copyright': 186,\n",
       " 'thank': 187,\n",
       " 'contribs': 188,\n",
       " 'unspecified': 189,\n",
       " 'source': 190,\n",
       " 'noticed': 191,\n",
       " 'file': 192,\n",
       " 'currently': 193,\n",
       " 'specify': 194,\n",
       " 'created': 195,\n",
       " 'content': 196,\n",
       " 'status': 197,\n",
       " 'unclear': 198,\n",
       " 'create': 199,\n",
       " 'owner': 200,\n",
       " 'obtained': 201,\n",
       " 'website': 202,\n",
       " 'taken': 203,\n",
       " 'together': 204,\n",
       " 'restatement': 205,\n",
       " 'term': 206,\n",
       " 'usually': 207,\n",
       " 'sufficient': 208,\n",
       " 'however': 209,\n",
       " 'holder': 210,\n",
       " 'different': 211,\n",
       " 'publisher': 212,\n",
       " 'acknowledged': 213,\n",
       " 'adding': 214,\n",
       " 'add': 215,\n",
       " 'proper': 216,\n",
       " 'licensing': 217,\n",
       " 'tag': 218,\n",
       " 'already': 219,\n",
       " 'createdtook': 220,\n",
       " 'picture': 221,\n",
       " 'audio': 222,\n",
       " 'video': 223,\n",
       " 'release': 224,\n",
       " 'gfdl': 225,\n",
       " 'believe': 226,\n",
       " 'meet': 227,\n",
       " 'wikipediafair': 228,\n",
       " 'wikipediaimage': 229,\n",
       " 'tagsfair': 230,\n",
       " 'see': 231,\n",
       " 'full': 232,\n",
       " 'tagged': 233,\n",
       " 'following': 234,\n",
       " 'unsourced': 235,\n",
       " 'untagged': 236,\n",
       " 'copyrighted': 237,\n",
       " 'nonfree': 238,\n",
       " 'license': 239,\n",
       " 'per': 240,\n",
       " 'hour': 241,\n",
       " 'bbq': 242,\n",
       " 'u': 243,\n",
       " 'discus': 244,\n",
       " 'itmaybe': 245,\n",
       " 'phone': 246,\n",
       " 'exclusive': 247,\n",
       " 'group': 248,\n",
       " 'wp': 249,\n",
       " 'talibanswho': 250,\n",
       " 'good': 251,\n",
       " 'destroying': 252,\n",
       " 'selfappointed': 253,\n",
       " 'purist': 254,\n",
       " 'gang': 255,\n",
       " 'asks': 256,\n",
       " 'antisocial': 257,\n",
       " 'destructive': 258,\n",
       " 'noncontribution': 259,\n",
       " 'sityush': 260,\n",
       " 'clean': 261,\n",
       " 'behavior': 262,\n",
       " 'issue': 263,\n",
       " 'nonsensical': 264,\n",
       " 'warning': 265,\n",
       " 'start': 266,\n",
       " 'throwing': 267,\n",
       " 'accusation': 268,\n",
       " 'itselfmaking': 269,\n",
       " 'ad': 270,\n",
       " 'hominem': 271,\n",
       " 'attack': 272,\n",
       " 'going': 273,\n",
       " 'strengthen': 274,\n",
       " 'argument': 275,\n",
       " 'look': 276,\n",
       " 'like': 277,\n",
       " 'abusing': 278,\n",
       " 'power': 279,\n",
       " 'admin': 280,\n",
       " 'relevantthis': 281,\n",
       " 'probably': 282,\n",
       " 'single': 283,\n",
       " 'talked': 284,\n",
       " 'event': 285,\n",
       " 'int': 286,\n",
       " 'news': 287,\n",
       " 'late': 288,\n",
       " 'absence': 289,\n",
       " 'notable': 290,\n",
       " 'living': 291,\n",
       " 'expresident': 292,\n",
       " 'attend': 293,\n",
       " 'certainly': 294,\n",
       " 'dedicating': 295,\n",
       " 'aircracft': 296,\n",
       " 'carrier': 297,\n",
       " 'intend': 298,\n",
       " 'revert': 299,\n",
       " 'hope': 300,\n",
       " 'attracting': 301,\n",
       " 'attention': 302,\n",
       " 'willing': 303,\n",
       " 'throw': 304,\n",
       " 'quite': 305,\n",
       " 'liberally': 306,\n",
       " 'perhaps': 307,\n",
       " 'achieve': 308,\n",
       " 'level': 309,\n",
       " 'civility': 310,\n",
       " 'rational': 311,\n",
       " 'discussion': 312,\n",
       " 'topic': 313,\n",
       " 'resolve': 314,\n",
       " 'matter': 315,\n",
       " 'peacefully': 316,\n",
       " 'oh': 317,\n",
       " 'girl': 318,\n",
       " 'started': 319,\n",
       " 'nose': 320,\n",
       " 'belong': 321,\n",
       " 'yvesnimmo': 322,\n",
       " 'said': 323,\n",
       " 'situation': 324,\n",
       " 'settled': 325,\n",
       " 'apologized': 326,\n",
       " 'juelz': 327,\n",
       " 'santanas': 328,\n",
       " 'age': 329,\n",
       " 'santana': 330,\n",
       " 'year': 331,\n",
       " 'old': 332,\n",
       " 'came': 333,\n",
       " 'february': 334,\n",
       " 'th': 335,\n",
       " 'making': 336,\n",
       " 'song': 337,\n",
       " 'diplomat': 338,\n",
       " 'third': 339,\n",
       " 'neff': 340,\n",
       " 'signed': 341,\n",
       " 'cam': 342,\n",
       " 'label': 343,\n",
       " 'roc': 344,\n",
       " 'fella': 345,\n",
       " 'coming': 346,\n",
       " 'town': 347,\n",
       " 'yes': 348,\n",
       " 'born': 349,\n",
       " 'could': 350,\n",
       " 'older': 351,\n",
       " 'lloyd': 352,\n",
       " 'bank': 353,\n",
       " 'birthday': 354,\n",
       " 'passed': 355,\n",
       " 'homie': 356,\n",
       " 'death': 357,\n",
       " 'god': 358,\n",
       " 'forbid': 359,\n",
       " 'thinking': 360,\n",
       " 'equal': 361,\n",
       " 'caculator': 362,\n",
       " 'stop': 363,\n",
       " 'changing': 364,\n",
       " 'birth': 365,\n",
       " 'bye': 366,\n",
       " 'come': 367,\n",
       " 'comming': 368,\n",
       " 'back': 369,\n",
       " 'tosser': 370,\n",
       " 'redirect': 371,\n",
       " 'talkvoydan': 372,\n",
       " 'pop': 373,\n",
       " 'georgiev': 374,\n",
       " 'chernodrinski': 375,\n",
       " 'mitsurugi': 376,\n",
       " 'sense': 377,\n",
       " 'argue': 378,\n",
       " 'hindi': 379,\n",
       " 'ryo': 380,\n",
       " 'sakazakis': 381,\n",
       " 'mean': 382,\n",
       " 'bother': 383,\n",
       " 'writing': 384,\n",
       " 'something': 385,\n",
       " 'regarding': 386,\n",
       " 'posted': 387,\n",
       " 'acctually': 388,\n",
       " 'even': 389,\n",
       " 'better': 390,\n",
       " 'take': 391,\n",
       " 'closer': 392,\n",
       " 'premature': 393,\n",
       " 'wrestling': 394,\n",
       " 'catagory': 395,\n",
       " 'men': 396,\n",
       " 'surely': 397,\n",
       " 'besides': 398,\n",
       " 'delting': 399,\n",
       " 'recent': 400,\n",
       " 'read': 401,\n",
       " 'wpfilmplot': 402,\n",
       " 'editing': 403,\n",
       " 'film': 404,\n",
       " 'simply': 405,\n",
       " 'entirely': 406,\n",
       " 'many': 407,\n",
       " 'unnecessary': 408,\n",
       " 'detail': 409,\n",
       " 'bad': 410,\n",
       " 'damage': 411,\n",
       " 'yeah': 412,\n",
       " 'studying': 413,\n",
       " 'nowdeepu': 414,\n",
       " 'snowflake': 415,\n",
       " 'always': 416,\n",
       " 'symmetrical': 417,\n",
       " 'geometry': 418,\n",
       " 'stated': 419,\n",
       " 'six': 420,\n",
       " 'symmetric': 421,\n",
       " 'arm': 422,\n",
       " 'assertion': 423,\n",
       " 'true': 424,\n",
       " 'according': 425,\n",
       " 'kenneth': 426,\n",
       " 'libbrecht': 427,\n",
       " 'rather': 428,\n",
       " 'unattractive': 429,\n",
       " 'irregular': 430,\n",
       " 'crystal': 431,\n",
       " 'far': 432,\n",
       " 'common': 433,\n",
       " 'variety': 434,\n",
       " 'site': 435,\n",
       " 'get': 436,\n",
       " 'fact': 437,\n",
       " 'still': 438,\n",
       " 'decent': 439,\n",
       " 'number': 440,\n",
       " 'falsity': 441,\n",
       " 'forgive': 442,\n",
       " 'signpost': 443,\n",
       " 'september': 444,\n",
       " 'singlepage': 445,\n",
       " 'unsubscribe': 446,\n",
       " 'reconsidering': 447,\n",
       " 'st': 448,\n",
       " 'paragraph': 449,\n",
       " 'understand': 450,\n",
       " 'reason': 451,\n",
       " 'sure': 452,\n",
       " 'data': 453,\n",
       " 'necessarily': 454,\n",
       " 'wrong': 455,\n",
       " 'persuaded': 456,\n",
       " 'strategy': 457,\n",
       " 'introducing': 458,\n",
       " 'academic': 459,\n",
       " 'honor': 460,\n",
       " 'unhelpful': 461,\n",
       " 'approach': 462,\n",
       " 'sitting': 463,\n",
       " 'justice': 464,\n",
       " 'similarly': 465,\n",
       " 'enhanced': 466,\n",
       " 'change': 467,\n",
       " 'support': 468,\n",
       " 'view': 469,\n",
       " 'invite': 470,\n",
       " 'anyone': 471,\n",
       " 'revisit': 472,\n",
       " 'written': 473,\n",
       " 'pair': 474,\n",
       " 'jurist': 475,\n",
       " 'benjamin': 476,\n",
       " 'cardozo': 477,\n",
       " 'learned': 478,\n",
       " 'hand': 479,\n",
       " 'b': 480,\n",
       " 'john': 481,\n",
       " 'marshall': 482,\n",
       " 'harlan': 483,\n",
       " 'ii': 484,\n",
       " 'becomes': 485,\n",
       " 'current': 486,\n",
       " 'version': 487,\n",
       " 'either': 488,\n",
       " 'improved': 489,\n",
       " 'credential': 490,\n",
       " 'introductory': 491,\n",
       " 'help': 492,\n",
       " 'repeat': 493,\n",
       " 'wry': 494,\n",
       " 'kathleen': 495,\n",
       " 'sullivan': 496,\n",
       " 'stanford': 497,\n",
       " 'law': 498,\n",
       " 'suggests': 499,\n",
       " 'harvard': 500,\n",
       " 'faculty': 501,\n",
       " 'wonder': 502,\n",
       " 'antonin': 503,\n",
       " 'scalia': 504,\n",
       " 'avoided': 505,\n",
       " 'learning': 506,\n",
       " 'others': 507,\n",
       " 'managed': 508,\n",
       " 'grasp': 509,\n",
       " 'process': 510,\n",
       " 'judging': 511,\n",
       " 'anecdote': 512,\n",
       " 'gently': 513,\n",
       " 'illustrates': 514,\n",
       " 'le': 515,\n",
       " 'humorous': 516,\n",
       " 'stronger': 517,\n",
       " 'clarence': 518,\n",
       " 'thomas': 519,\n",
       " 'mention': 520,\n",
       " 'wanting': 521,\n",
       " 'return': 522,\n",
       " 'degree': 523,\n",
       " 'yale': 524,\n",
       " 'minimum': 525,\n",
       " 'questioning': 526,\n",
       " 'deserves': 527,\n",
       " 'reconsidered': 528,\n",
       " 'radial': 529,\n",
       " 'symmetry': 530,\n",
       " 'several': 531,\n",
       " 'extinct': 532,\n",
       " 'lineage': 533,\n",
       " 'included': 534,\n",
       " 'echinodermata': 535,\n",
       " 'bilateral': 536,\n",
       " 'homostelea': 537,\n",
       " 'asymmetrical': 538,\n",
       " 'cothurnocystis': 539,\n",
       " 'stylophora': 540,\n",
       " 'apologize': 541,\n",
       " 'reconciling': 542,\n",
       " 'knowledge': 543,\n",
       " 'done': 544,\n",
       " 'history': 545,\n",
       " 'study': 546,\n",
       " 'archaeology': 547,\n",
       " 'scan': 548,\n",
       " 'email': 549,\n",
       " 'translate': 550,\n",
       " 'mother': 551,\n",
       " 'child': 552,\n",
       " 'case': 553,\n",
       " 'michael': 554,\n",
       " 'jackson': 555,\n",
       " 'studied': 556,\n",
       " 'motif': 557,\n",
       " 'reasoning': 558,\n",
       " 'judged': 559,\n",
       " 'upon': 560,\n",
       " 'character': 561,\n",
       " 'harshly': 562,\n",
       " 'wacko': 563,\n",
       " 'jacko': 564,\n",
       " 'tell': 565,\n",
       " 'ignore': 566,\n",
       " 'incriminate': 567,\n",
       " 'continue': 568,\n",
       " 'refuting': 569,\n",
       " 'bullshit': 570,\n",
       " 'jayjg': 571,\n",
       " 'keep': 572,\n",
       " 'jun': 573,\n",
       " 'ok': 574,\n",
       " 'bit': 575,\n",
       " 'example': 576,\n",
       " 'base': 577,\n",
       " 'duck': 578,\n",
       " 'barnstar': 579,\n",
       " 'life': 580,\n",
       " 'star': 581,\n",
       " 'post': 582,\n",
       " 'block': 583,\n",
       " 'expires': 584,\n",
       " 'funny': 585,\n",
       " 'thing': 586,\n",
       " 'uncivil': 587,\n",
       " 'heading': 588,\n",
       " 'fight': 589,\n",
       " 'freedom': 590,\n",
       " 'contain': 591,\n",
       " 'praise': 592,\n",
       " 'looked': 593,\n",
       " 'month': 594,\n",
       " 'ago': 595,\n",
       " 'much': 596,\n",
       " 'able': 597,\n",
       " 'quickly': 598,\n",
       " 'text': 599,\n",
       " 'hard': 600,\n",
       " 'drive': 601,\n",
       " 'meaning': 602,\n",
       " 'updating': 603,\n",
       " 'sound': 604,\n",
       " 'time': 605,\n",
       " 'generating': 606,\n",
       " 'interest': 607,\n",
       " 'spent': 608,\n",
       " 'four': 609,\n",
       " 'drum': 610,\n",
       " 'freely': 611,\n",
       " 'licensed': 612,\n",
       " 'length': 613,\n",
       " 'classical': 614,\n",
       " 'music': 615,\n",
       " 'unfortunately': 616,\n",
       " 'attempt': 617,\n",
       " 'failed': 618,\n",
       " 'effectively': 619,\n",
       " 'wikiproject': 620,\n",
       " 'interested': 621,\n",
       " 'wikipediatalkwikiprojectclassicalmusicarchive': 622,\n",
       " 'needhelp': 623,\n",
       " 'wikipediatalkwikiprojectmusicarchive': 624,\n",
       " 'icouldusesomehelpwikipediatalkwikiprojectmusicarchive': 625,\n",
       " 'raulbot': 626,\n",
       " 'candthemusiclist': 627,\n",
       " 'given': 628,\n",
       " 'featured': 629,\n",
       " 'digg': 630,\n",
       " 'got': 631,\n",
       " 'diggs': 632,\n",
       " 'impressive': 633,\n",
       " 'subpages': 634,\n",
       " 'rfa': 635,\n",
       " 'noseptembers': 636,\n",
       " 'difference': 637,\n",
       " 'elc': 638,\n",
       " 'surprised': 639,\n",
       " 'left': 640,\n",
       " 'tc': 641,\n",
       " 'straw': 642,\n",
       " 'never': 643,\n",
       " 'claimed': 644,\n",
       " 'odonohue': 645,\n",
       " 'position': 646,\n",
       " 'practitioner': 647,\n",
       " 'researcher': 648,\n",
       " 'field': 649,\n",
       " 'ignored': 650,\n",
       " 'dsm': 651,\n",
       " 'exactly': 652,\n",
       " 'quote': 653,\n",
       " 'say': 654,\n",
       " 'agrees': 655,\n",
       " 'combating': 656,\n",
       " 'notion': 657,\n",
       " 'absurd': 658,\n",
       " 'part': 659,\n",
       " 'claim': 660,\n",
       " 'pedophilia': 661,\n",
       " 'sexual': 662,\n",
       " 'orientation': 663,\n",
       " 'hold': 664,\n",
       " 'unfair': 665,\n",
       " 'call': 666,\n",
       " 'disorder': 667,\n",
       " 'divided': 668,\n",
       " 'end': 669,\n",
       " 'day': 670,\n",
       " 'value': 671,\n",
       " 'judgment': 672,\n",
       " 'cantor': 673,\n",
       " 'pointed': 674,\n",
       " 'earlier': 675,\n",
       " 'thread': 676,\n",
       " 'scientific': 677,\n",
       " 'judgement': 678,\n",
       " 'choose': 679,\n",
       " 'clearly': 680,\n",
       " 'pretend': 681,\n",
       " 'basis': 682,\n",
       " 'mainland': 683,\n",
       " 'asia': 684,\n",
       " 'includes': 685,\n",
       " 'lower': 686,\n",
       " 'basin': 687,\n",
       " 'china': 688,\n",
       " 'yangtze': 689,\n",
       " 'river': 690,\n",
       " 'korea': 691,\n",
       " 'fine': 692,\n",
       " 'found': 693,\n",
       " 'citation': 694,\n",
       " 'comprehensive': 695,\n",
       " 'dna': 696,\n",
       " 'hammer': 697,\n",
       " 'generarizations': 698,\n",
       " 'speculation': 699,\n",
       " 'yayoi': 700,\n",
       " 'culture': 701,\n",
       " 'brought': 702,\n",
       " 'japan': 703,\n",
       " 'migrant': 704,\n",
       " 'trace': 705,\n",
       " 'root': 706,\n",
       " 'southeast': 707,\n",
       " 'asiasouth': 708,\n",
       " 'describes': 709,\n",
       " 'migration': 710,\n",
       " 'based': 711,\n",
       " 'osry': 712,\n",
       " 'gene': 713,\n",
       " 'close': 714,\n",
       " 'haplogroups': 715,\n",
       " 'om': 716,\n",
       " 'reiterates': 717,\n",
       " 'entire': 718,\n",
       " 'haplogroup': 719,\n",
       " 'proposed': 720,\n",
       " 'asian': 721,\n",
       " 'origin': 722,\n",
       " 'definition': 723,\n",
       " 'southern': 724,\n",
       " 'hypothesizes': 725,\n",
       " 'dispersal': 726,\n",
       " 'neolithic': 727,\n",
       " 'farmer': 728,\n",
       " 'eventually': 729,\n",
       " 'concluding': 730,\n",
       " 'state': 731,\n",
       " 'propose': 732,\n",
       " 'chromosome': 733,\n",
       " 'descend': 734,\n",
       " 'prehistoric': 735,\n",
       " 'southeastern': 736,\n",
       " 'agriculture': 737,\n",
       " 'region': 738,\n",
       " 'global': 739,\n",
       " 'sample': 740,\n",
       " 'consisted': 741,\n",
       " 'male': 742,\n",
       " 'population': 743,\n",
       " 'including': 744,\n",
       " 'sampled': 745,\n",
       " 'across': 746,\n",
       " 'japanese': 747,\n",
       " 'archipelago': 748,\n",
       " 'pretty': 749,\n",
       " 'everyone': 750,\n",
       " 'warren': 751,\n",
       " 'countysurrounding': 752,\n",
       " 'glen': 753,\n",
       " 'fall': 754,\n",
       " 'hospital': 755,\n",
       " 'qualifies': 756,\n",
       " 'native': 757,\n",
       " 'rachel': 758,\n",
       " 'ray': 759,\n",
       " 'actually': 760,\n",
       " 'lake': 761,\n",
       " 'luzerne': 762,\n",
       " 'preceding': 763,\n",
       " 'unsigned': 764,\n",
       " 'comment': 765,\n",
       " 'added': 766,\n",
       " 'august': 767,\n",
       " 'hi': 768,\n",
       " 'explicit': 769,\n",
       " 'fenian': 770,\n",
       " 'editwarring': 771,\n",
       " 'giant': 772,\n",
       " 'causeway': 773,\n",
       " 'terrorism': 774,\n",
       " 'notability': 775,\n",
       " 'rurika': 776,\n",
       " 'kasuga': 777,\n",
       " 'placed': 778,\n",
       " 'speedily': 779,\n",
       " 'person': 780,\n",
       " 'people': 781,\n",
       " 'band': 782,\n",
       " 'club': 783,\n",
       " 'company': 784,\n",
       " 'web': 785,\n",
       " 'indicate': 786,\n",
       " 'assert': 787,\n",
       " 'guideline': 788,\n",
       " 'generally': 789,\n",
       " 'accepted': 790,\n",
       " 'contest': 791,\n",
       " 'tagging': 792,\n",
       " 'existing': 793,\n",
       " 'db': 794,\n",
       " 'leave': 795,\n",
       " 'explaining': 796,\n",
       " 'hesitate': 797,\n",
       " 'confirm': 798,\n",
       " 'check': 799,\n",
       " 'biography': 800,\n",
       " 'feel': 801,\n",
       " 'free': 802,\n",
       " 'lead': 803,\n",
       " 'briefly': 804,\n",
       " 'summarize': 805,\n",
       " 'armenia': 806,\n",
       " 'necessary': 807,\n",
       " 'sentence': 808,\n",
       " 'redundant': 809,\n",
       " 'welcome': 810,\n",
       " 'tfd': 811,\n",
       " 'eced': 812,\n",
       " 'responded': 813,\n",
       " 'without': 814,\n",
       " 'seeing': 815,\n",
       " 'response': 816,\n",
       " 'saw': 817,\n",
       " 'mine': 818,\n",
       " 'tcwpchicagowpfour': 819,\n",
       " 'gay': 820,\n",
       " 'antisemmitian': 821,\n",
       " 'archangel': 822,\n",
       " 'white': 823,\n",
       " 'tiger': 824,\n",
       " 'meow': 825,\n",
       " 'greetingshhh': 826,\n",
       " 'uh': 827,\n",
       " 'two': 828,\n",
       " 'way': 829,\n",
       " 'erased': 830,\n",
       " 'ww': 831,\n",
       " 'holocaust': 832,\n",
       " 'brutally': 833,\n",
       " 'slaying': 834,\n",
       " 'jew': 835,\n",
       " 'gaysgypsysslavsanyone': 836,\n",
       " 'antisemitian': 837,\n",
       " 'shave': 838,\n",
       " 'head': 839,\n",
       " 'bald': 840,\n",
       " 'skinhead': 841,\n",
       " 'meeting': 842,\n",
       " 'doubt': 843,\n",
       " 'bible': 844,\n",
       " 'homosexuality': 845,\n",
       " 'deadly': 846,\n",
       " 'sin': 847,\n",
       " 'pentagram': 848,\n",
       " 'tatoo': 849,\n",
       " 'forehead': 850,\n",
       " 'satanistic': 851,\n",
       " 'mass': 852,\n",
       " 'pal': 853,\n",
       " 'last': 854,\n",
       " 'fucking': 855,\n",
       " 'appreciate': 856,\n",
       " 'nazi': 857,\n",
       " 'shwain': 858,\n",
       " 'wish': 859,\n",
       " 'anymore': 860,\n",
       " 'beware': 861,\n",
       " 'dark': 862,\n",
       " 'side': 863,\n",
       " 'fuck': 864,\n",
       " 'filthy': 865,\n",
       " 'as': 866,\n",
       " 'dry': 867,\n",
       " 'screwed': 868,\n",
       " 'dominance': 869,\n",
       " 'bow': 870,\n",
       " 'almighty': 871,\n",
       " 'administrator': 872,\n",
       " 'play': 873,\n",
       " 'outsidewith': 874,\n",
       " 'mom': 875,\n",
       " 'lisak': 876,\n",
       " 'criticism': 877,\n",
       " 'present': 878,\n",
       " 'conforms': 879,\n",
       " 'npv': 880,\n",
       " 'rule': 881,\n",
       " 'neutral': 882,\n",
       " 'begin': 883,\n",
       " 'offer': 884,\n",
       " 'polygraph': 885,\n",
       " 'concerned': 886,\n",
       " 'result': 887,\n",
       " 'shock': 888,\n",
       " 'complainant': 889,\n",
       " 'lie': 890,\n",
       " 'uncovered': 891,\n",
       " 'recantation': 892,\n",
       " 'perfectly': 893,\n",
       " 'valid': 894,\n",
       " 'telling': 895,\n",
       " 'truth': 896,\n",
       " 'machine': 897,\n",
       " 'investigator': 898,\n",
       " 'kanins': 899,\n",
       " 'research': 900,\n",
       " 'followup': 901,\n",
       " 'recanted': 902,\n",
       " 'story': 903,\n",
       " 'possible': 904,\n",
       " 'verify': 905,\n",
       " 'false': 906,\n",
       " 'matched': 907,\n",
       " 'accused': 908,\n",
       " 'happened': 909,\n",
       " 'arguing': 910,\n",
       " 'respected': 911,\n",
       " 'phd': 912,\n",
       " 'baseless': 913,\n",
       " 'kanin': 914,\n",
       " 'agree': 915,\n",
       " 'though': 916,\n",
       " 'ammended': 917,\n",
       " 'appropriate': 918,\n",
       " 'notabilitysignificance': 919,\n",
       " 'lazy': 920,\n",
       " 'stalking': 921,\n",
       " 'absolute': 922,\n",
       " 'rubbish': 923,\n",
       " 'serf': 924,\n",
       " 'aggravate': 925,\n",
       " 'assumed': 926,\n",
       " 'faith': 927,\n",
       " 'intention': 928,\n",
       " 'suggested': 929,\n",
       " 'seen': 930,\n",
       " 'suggest': 931,\n",
       " 'might': 932,\n",
       " 'ulterior': 933,\n",
       " 'motive': 934,\n",
       " 'massadding': 935,\n",
       " 'ever': 936,\n",
       " 'administrative': 937,\n",
       " 'mentioned': 938,\n",
       " 'role': 939,\n",
       " 'party': 940,\n",
       " 'disagreement': 941,\n",
       " 'rate': 942,\n",
       " 'conflict': 943,\n",
       " 'thus': 944,\n",
       " 'extend': 945,\n",
       " 'toward': 946,\n",
       " 'spurious': 947,\n",
       " 'unfounded': 948,\n",
       " 'chatspy': 949,\n",
       " 'jmabel': 950,\n",
       " 'regard': 951,\n",
       " 'predominant': 952,\n",
       " 'scholary': 953,\n",
       " 'consensus': 954,\n",
       " 'allegedly': 955,\n",
       " 'despite': 956,\n",
       " 'rhetoric': 957,\n",
       " 'fascism': 958,\n",
       " 'functioned': 959,\n",
       " 'consistently': 960,\n",
       " 'rightwing': 961,\n",
       " 'force': 962,\n",
       " 'aware': 963,\n",
       " 'owning': 964,\n",
       " 'numerous': 965,\n",
       " 'book': 966,\n",
       " 'developed': 967,\n",
       " 'scholar': 968,\n",
       " 'manner': 969,\n",
       " 'bias': 970,\n",
       " 'roger': 971,\n",
       " 'griffin': 972,\n",
       " 'hamish': 973,\n",
       " 'mcdonald': 974,\n",
       " 'eatwell': 975,\n",
       " 'zeev': 976,\n",
       " 'sternhell': 977,\n",
       " 'recongise': 978,\n",
       " 'show': 979,\n",
       " 'dissenter': 980,\n",
       " 'seem': 981,\n",
       " 'absoutely': 982,\n",
       " 'leftist': 983,\n",
       " 'connection': 984,\n",
       " 'radical': 985,\n",
       " 'right': 986,\n",
       " 'system': 987,\n",
       " 'street': 988,\n",
       " 'socialist': 989,\n",
       " 'put': 990,\n",
       " 'distance': 991,\n",
       " 'movement': 992,\n",
       " 'course': 993,\n",
       " 'educated': 994,\n",
       " 'foremost': 995,\n",
       " 'expert': 996,\n",
       " 'former': 997,\n",
       " 'member': 998,\n",
       " 'communist': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function create_vocab()\n",
    "word_to_index, index_to_word = create_vocab(data,'lemmatized_comment')\n",
    "\n",
    "# Get the size of the vocabulary\n",
    "vocab_size = len(word_to_index)\n",
    "\n",
    "print('Vocabulary size = ',vocab_size)\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Local dictionary is made that have the unique words of data corresponding to their indices (by row)\n",
    "- word to index dictionary that have the word as the index and corresponds to its unique number \n",
    "- index to word dictionary that have the index as the unique numbers correspond to their words\n",
    "- The vocabulary has 204909 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padding function is designed to prepare the lemmatized comment text to get into the transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate the mean of the max word counts of all rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this let us know the maximum sequence length the padding shoulf have to cover most of the sequence in text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the maximum word counts of all rows:  33.57228315687307\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count the number of words in a text\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# Calculate the maximum word count in each row and store it in a list\n",
    "max_word_counts = data['lemmatized_comment'].apply(count_words)\n",
    "\n",
    "# Calculate the mean of the maximum word counts of all rows\n",
    "mean_max_word_count = max_word_counts.mean()\n",
    "\n",
    "print(\"Mean of the maximum word counts of all rows: \",mean_max_word_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take 50 as a maximum sequence length to cover a good sequence of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(data,column, word_to_index, max_seq_length=50):\n",
    "\n",
    "    # Create an empty list to store sequences\n",
    "    sequences = []\n",
    "\n",
    "    # Iterate through each comment in the column\n",
    "    for comment in data[column]:\n",
    "\n",
    "        # Split the comment into words\n",
    "        words = comment.split()\n",
    "\n",
    "        # Convert words to their corresponding indices using the word-to-index dictionary made before\n",
    "        sequence = [word_to_index.get(word, word_to_index[\"<PAD>\"]) for word in words]\n",
    "\n",
    "        # Pad the sequence with '<PAD>' indices to reach the maximum sequence length mentioned (50)\n",
    "        sequence = sequence[:max_seq_length]\n",
    "        sequence.extend([word_to_index[\"<PAD>\"]] * (max_seq_length - len(sequence)))\n",
    "\n",
    "        # Append the sequence to the list of sequences (The empty list i created above)\n",
    "        sequences.append(sequence)\n",
    "\n",
    "    # Convert the list of sequences array\n",
    "    padded_sequences = np.array(sequences)\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "      <th>Padding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 20, 31, 32, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>[92, 93, 94, 95, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>[1204, 605, 2774, 469, 1103, 10165, 6301, 1153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>[8172, 2702, 586, 990, 20, 21, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>[89207, 10570, 49, 82, 7284, 1891, 24476, 3595...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "      <td>[276, 277, 760, 990, 182, 74, 487, 178, 276, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>[35, 62, 450, 333, 2321, 410, 986, 1677, 1681,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157637 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "159566      0             0        0       0       0              0   \n",
       "159567      0             0        0       0       0              0   \n",
       "159568      0             0        0       0       0              0   \n",
       "159569      0             0        0       0       0              0   \n",
       "159570      0             0        0       0       0              0   \n",
       "\n",
       "                                        tokenized_comment  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "159566  [second, time, asking, view, completely, contr...   \n",
       "159567        [ashamed, horrible, thing, put, talk, page]   \n",
       "159568  [spitzer, umm, actual, article, prostitution, ...   \n",
       "159569  [looks, like, actually, put, speedy, first, ve...   \n",
       "159570  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                       lemmatized_comment  \\\n",
       "0       explanation edits made username hardcore metal...   \n",
       "1       daww match background colour seemingly stuck t...   \n",
       "2       hey man really trying edit war guy constantly ...   \n",
       "3       make real suggestion improvement wondered sect...   \n",
       "4                           sir hero chance remember page   \n",
       "...                                                   ...   \n",
       "159566  second time asking view completely contradicts...   \n",
       "159567               ashamed horrible thing put talk page   \n",
       "159568  spitzer umm actual article prostitution ring c...   \n",
       "159569  look like actually put speedy first version de...   \n",
       "159570  really think understand came idea bad right aw...   \n",
       "\n",
       "                                                  Padding  \n",
       "0       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
       "1       [24, 25, 26, 27, 28, 29, 30, 20, 31, 32, 0, 0,...  \n",
       "2       [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...  \n",
       "3       [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 6...  \n",
       "4       [92, 93, 94, 95, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "...                                                   ...  \n",
       "159566  [1204, 605, 2774, 469, 1103, 10165, 6301, 1153...  \n",
       "159567  [8172, 2702, 586, 990, 20, 21, 0, 0, 0, 0, 0, ...  \n",
       "159568  [89207, 10570, 49, 82, 7284, 1891, 24476, 3595...  \n",
       "159569  [276, 277, 760, 990, 182, 74, 487, 178, 276, 0...  \n",
       "159570  [35, 62, 450, 333, 2321, 410, 986, 1677, 1681,...  \n",
       "\n",
       "[157637 rows x 11 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function padding()\n",
    "padded_sequences = padding(data,'lemmatized_comment', word_to_index)\n",
    "\n",
    "# Create a new column called ('Padding') and store padded_sequences in it\n",
    "data['Padding'] = list(padded_sequences)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padding function converts the lemmatized comment text into numerical sequences with word-to-index (got from the vocabulary made) and sequence length (50) and return this sequence as array for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New column label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I made new column that take the conditions i made above to easily classify data\n",
    "\n",
    "I made this to convert this problem to a binary classification label to solve the imbalcing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>tokenized_comment</th>\n",
       "      <th>lemmatized_comment</th>\n",
       "      <th>Padding</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 20, 31, 32, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>[92, 93, 94, 95, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>[1204, 605, 2774, 469, 1103, 10165, 6301, 1153...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>[8172, 2702, 586, 990, 20, 21, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>[89207, 10570, 49, 82, 7284, 1891, 24476, 3595...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "      <td>[276, 277, 760, 990, 182, 74, 487, 178, 276, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>[35, 62, 450, 333, 2321, 410, 986, 1677, 1681,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157637 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "                                        tokenized_comment  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "159566  [second, time, asking, view, completely, contr...   \n",
       "159567        [ashamed, horrible, thing, put, talk, page]   \n",
       "159568  [spitzer, umm, actual, article, prostitution, ...   \n",
       "159569  [looks, like, actually, put, speedy, first, ve...   \n",
       "159570  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                       lemmatized_comment  \\\n",
       "0       explanation edits made username hardcore metal...   \n",
       "1       daww match background colour seemingly stuck t...   \n",
       "2       hey man really trying edit war guy constantly ...   \n",
       "3       make real suggestion improvement wondered sect...   \n",
       "4                           sir hero chance remember page   \n",
       "...                                                   ...   \n",
       "159566  second time asking view completely contradicts...   \n",
       "159567               ashamed horrible thing put talk page   \n",
       "159568  spitzer umm actual article prostitution ring c...   \n",
       "159569  look like actually put speedy first version de...   \n",
       "159570  really think understand came idea bad right aw...   \n",
       "\n",
       "                                                  Padding  label  \n",
       "0       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...      0  \n",
       "1       [24, 25, 26, 27, 28, 29, 30, 20, 31, 32, 0, 0,...      0  \n",
       "2       [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...      0  \n",
       "3       [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 6...      0  \n",
       "4       [92, 93, 94, 95, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0...      0  \n",
       "...                                                   ...    ...  \n",
       "159566  [1204, 605, 2774, 469, 1103, 10165, 6301, 1153...      0  \n",
       "159567  [8172, 2702, 586, 990, 20, 21, 0, 0, 0, 0, 0, ...      0  \n",
       "159568  [89207, 10570, 49, 82, 7284, 1891, 24476, 3595...      0  \n",
       "159569  [276, 277, 760, 990, 182, 74, 487, 178, 276, 0...      0  \n",
       "159570  [35, 62, 450, 333, 2321, 410, 986, 1677, 1681,...      0  \n",
       "\n",
       "[157637 rows x 6 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new \"label\" column based on conditions\n",
    "data['label'] = 0  # Initialize all labels to 0\n",
    "data.loc[onesData, 'label'] = 1  # Set labels to 1 for the onesData\n",
    "\n",
    "# Drop the other labels as they are not important now\n",
    "data.drop(['toxic','severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        Now data is cleaned and prepared to enter the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pytorch, the steps are:\n",
    "- Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
    "- Build the Encoder layer\n",
    "- Create a complete Transformer model\n",
    "- Split data into train and test\n",
    "- Train the model\n",
    "- Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it's a classification model, we don't need a decoder layer\n",
    "\n",
    "A decoder is in sequence-to-sequence models or autoencoders for tasks like machine translation or text generation, that's why i won't use it\n",
    "\n",
    "I'll use only the encoder layer as i'm directly mapping input data to the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultiHeadAttention mechanism is a fundamental building block of Transformer model\n",
    "\n",
    "The input sequence is linearly transformed into three separate sets of vectors (keys, queries, and values) for each attention head\n",
    "\n",
    "Each attention head measures the similarity between the queries and keys and computes a set of attention weights \n",
    "\n",
    "These weights determine how much each position in the input should contribute to the output\n",
    "\n",
    "The outputs of all attention heads are then combined to produce the final output, which called [ weighted sum of the values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Initialize the MultiHeadAttention layer\n",
    "        self.d_model = d_model      # Dimensionality of input data\n",
    "        self.n_heads = n_heads      # Number of attention heads\n",
    "        self.head_dim = d_model // n_heads      # Dimensionality of each attention head\n",
    "\n",
    "        # Linear transformations for Key, Query, Value, and the Output\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \n",
    "        # Get the batch size from the input tensor\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # Project the input Key, Query, and Value tensors using the linear transformation\n",
    "        K = self.W_K(K)\n",
    "        Q = self.W_Q(Q)\n",
    "        V = self.W_V(V)\n",
    "\n",
    "        # Reshape the projected tensors to support multi-head attention\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim)\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim)\n",
    "\n",
    "        # Compute the dot product of Q and K for each head\n",
    "            # bqhd, bkhd (for performing tensor contractions):\n",
    "                # b: batch size\n",
    "                # q: query dimentions \n",
    "                # h: number of attention heads\n",
    "                # d: dimention of each head\n",
    "                # k: key dimentions\n",
    "            # bhqk (the output tensor):\n",
    "                # b: batch size (the same)\n",
    "                # h: number of attention heads\n",
    "                # q: query dimentions (swapped with k)\n",
    "                # k: key dimentions (swapped with q)\n",
    "        QK = torch.einsum('bqhd,bkhd->bhqk', Q, K)\n",
    "\n",
    "        # Scale the dot products by the square root of the head dimension\n",
    "        scores = QK / torch.sqrt(torch.tensor(self.head_dim).float())\n",
    "\n",
    "        # Apply mask to the attention scores to mask out certain values \n",
    "        if mask is not None:\n",
    "            # Expand the mask dimensions to match the shape of the attention scores\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "         # Set certain elements of attention scores to a low value (-1e9) and ignore them in computations\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "        # Compute attention weights using softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Calculates the weighted sum of values based on the attention scores for each head \n",
    "                # First input tensor (bhqv):\n",
    "                    # b: batch size\n",
    "                    # h: number of attetion heads\n",
    "                    # q: query dimentions\n",
    "                    # v: value dimentions\n",
    "                # Second input tensor (bvhd):\n",
    "                    # b: batch size\n",
    "                    # v: value dimentions\n",
    "                    # h: number of attention heads\n",
    "                    # d: dimention of each attention head\n",
    "                # Output tensor (bqhd):\n",
    "                    # b: batch size\n",
    "                    # q: query dimentions\n",
    "                    # h: number of attention heads\n",
    "                    # d: dimention of each head\n",
    "        out = torch.einsum('bhqv,bvhd->bqhd', attention_weights, V)\n",
    "\n",
    "        # Reshape the output \n",
    "        out = out.reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Put the output in the output linear transformation\n",
    "        out = self.W_O(out)\n",
    "\n",
    "        # Return the final output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This class introduces changes in dimensions to each position's representation in the input sequence\n",
    " \n",
    " It allows the model to capture complex relationships within the data while maintaining the overall structure of the original representation\n",
    "\n",
    " The feedforward neural network has 2 fully connected layers, the input go through the first layer through RELU activation function then pass through the second layer to produce the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # Initialize the first fully connected layer \n",
    "        # Input size is the hidden stated dimentions (d_model) \n",
    "        # Output size is the feed forward dimentions (d_ff)\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # Initialize the second fully connected layer \n",
    "        # Input size is the feed forward dimentions (d_model) \n",
    "        # Output size is the hidden stated dimentions (d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply ReLU activation function to the first layer output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # move the output of the first layer to the second layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Return the final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class adds positional information to the input data in a way that the transformer model can use, enabling it to capture sequential patterns and relationships when processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Initialize a zeros positional encoding matrix of shape (max_seq_len, d_model)  \n",
    "        self.pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        # Create a tensor containing values from 0 to max_seq_len - 1\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Calculate exponents and put it in tensor to create sinusoidal positional encodings\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate sine and cosine positional encodings \n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term) \n",
    "        \n",
    "        # Add an additional dimension to make it suitable with the model input\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Add the positional encodings to the input x\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder Layer consists of a self-attention mechanism with multiple heads, a feed-forward neural network, and layer normalization after each operation\n",
    "\n",
    "The Transformer Encoder stacks multiple Encoder Layers to create the encoder, making it suitable to the sequence modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Self-attention mechanism with multiple heads\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        \n",
    "        # Feed-forward neural network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer normalization after self-attention\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Layer normalization after feed-forward\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        # Multi-head self-attention operation\n",
    "        attention_output = self.self_attention(x, x, x, mask)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        x = x + attention_output\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-forward neural network operation\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        \n",
    "        # Residual connection and layer normalization\n",
    "        x = x + feed_forward_output\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        # Return the final output \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, d_model, n_heads, d_ff):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Stacking multiple encoder layers to create the encoder\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            \n",
    "            # Forward passes the input through each Encoder Layer\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Return the final output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class combines all the above building blocks to create the full transformer architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers, d_model, n_heads, d_ff, input_vocab_size, max_seq_len, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Embedding layer to convert input tokens to dense vector\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding to provide positional information to the model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # Stacking multiple encoder layers to create the encoder\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, n_heads, d_ff)\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        # Embedding input tokens\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Add positional encoding to the embedded tokens\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # Embedded tokens pass through the encoder\n",
    "        x = self.encoder(x, mask)\n",
    "\n",
    "        # Global average pooling to obtain a fixed-size representation of the sequence\n",
    "        x = x.mean(dim=1)  \n",
    "\n",
    "        # The fixed-size representation pass through the fully connected layer for classification\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # Return the final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the Transformer architecture is completed and ready to implement it for train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts array padded_sequences to a pyTorch tensor X and the .long() method is used to ensure that the tensor contains integer values\n",
    "X = torch.from_numpy(padded_sequences).long()\n",
    "\n",
    "# This line converts the values of column label (which is an array) to a pytorch tensor y and the .long() method is used to ensure that the tensor contains integer values\n",
    "y = torch.from_numpy(data['label'].values).long()  \n",
    "\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Number of samples in each batch\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader for training, validation, and testing\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters - Implement the model, loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After hyperparameter tuning, these are the best hyperparameters i reached to give the best accuracy in train and test for the model\n",
    "\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "input_vocab_size = vocab_size  \n",
    "max_seq_len = 50    # Same as the max_seq_len of padding\n",
    "num_classes = 2     # As it is binary classification\n",
    "learning_rate = 0.0001\n",
    "patience = 3        # Number of times the model will continue training if there is no improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the transformer model\n",
    "model = Transformer(num_layers, d_model, n_heads, d_ff, input_vocab_size, max_seq_len, num_classes)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the optinmizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience):\n",
    "\n",
    "    # train model\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the best validation loss to a large value (positive infinity)\n",
    "    best_valid_loss = float('inf')  \n",
    "\n",
    "    # Initialize a counter for consecutive epochs with no improvement\n",
    "    consecutive_no_improvement = 0  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Initialize the loss, correct predictions, and total samples in each epoch\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader, 1):\n",
    "\n",
    "            # Initialize gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Create mask to ignore padding tokens \n",
    "            mask = (inputs != 0)\n",
    "\n",
    "            # Pass inputs and mask through the model to predict the output\n",
    "            outputs = model(inputs, mask)\n",
    "\n",
    "            # Calculate loss between the predicted output and the target\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backpropagate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the parameters in the model\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Add the loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get the predicted labels\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Calculate the correct predictions\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "            # Count the total samples in the batch\n",
    "            total_samples += targets.size(0)\n",
    "            \n",
    "            print(f\"\\rEpoch {epoch + 1}/{num_epochs}\", end='', flush=True)\n",
    "        \n",
    "        # Calculate epoch loss and accuracy\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct_predictions / total_samples\n",
    "        \n",
    "    ## Validation\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize the validation loss, the correct predictions, and total samples\n",
    "        valid_loss = 0.0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            for valid_inputs, valid_targets in val_loader:\n",
    "\n",
    "                # Create mask\n",
    "                valid_mask = (valid_inputs != 0)\n",
    "\n",
    "                # Pass validation inputs and mask through the model to predict the output\n",
    "                valid_outputs = model(valid_inputs, valid_mask)\n",
    "\n",
    "                # Calculate loss between the predicted output and the target\n",
    "                valid_loss_batch = criterion(valid_outputs, valid_targets)\n",
    "\n",
    "                # add the loss\n",
    "                valid_loss += valid_loss_batch.item()\n",
    "                \n",
    "                # Get the predicted labels\n",
    "                _, valid_predicted = torch.max(valid_outputs, 1)\n",
    "\n",
    "                # Calculate the correct predictions\n",
    "                valid_correct += (valid_predicted == valid_targets).sum().item()\n",
    "\n",
    "                # Count the total samples in the batch\n",
    "                valid_total += valid_targets.size(0)\n",
    "\n",
    "        # Calculate epoch loss and accuracy for validation\n",
    "        valid_accuracy = 100 * valid_correct / valid_total\n",
    "        avg_valid_loss = valid_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"\\rEpoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%, \" \\\n",
    "              f\"Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "        \n",
    "        # Check if the validation loss has improved\n",
    "\n",
    "        # If the average validation loss is better than the current best validation loss, the best validation loss is updated\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            consecutive_no_improvement = 0      # the consecutive no improvent is reset to 0 as it improved\n",
    "        else:\n",
    "            consecutive_no_improvement += 1     # Add the counter of no improvements by 1\n",
    "        \n",
    "        # If there is no improvement for 'patience' consecutive epochs, stop training\n",
    "        if consecutive_no_improvement >= patience:\n",
    "            print(\"-----------\\nNo improvements in validation loss\\nTraining Stopped\")\n",
    "            break\n",
    "        \n",
    "        # Train model again\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.1755, Train Accuracy: 94.44%, Valid Loss: 0.1503, Valid Accuracy: 94.99%\n",
      "Epoch 2/10, Train Loss: 0.1158, Train Accuracy: 95.95%, Valid Loss: 0.1395, Valid Accuracy: 95.20%\n",
      "Epoch 3/10, Train Loss: 0.0808, Train Accuracy: 97.08%, Valid Loss: 0.1481, Valid Accuracy: 95.17%\n",
      "Epoch 4/10, Train Loss: 0.0472, Train Accuracy: 98.28%, Valid Loss: 0.1645, Valid Accuracy: 94.86%\n",
      "Epoch 5/10, Train Loss: 0.0282, Train Accuracy: 99.03%, Valid Loss: 0.1949, Valid Accuracy: 94.72%\n",
      "-----------\n",
      "No improvements in validation loss\n",
      "Training Stopped\n"
     ]
    }
   ],
   "source": [
    "# Call the train function with 10 epochs\n",
    "num_epochs = 10\n",
    "train(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In epoch 2, both the training loss and validation loss decreased, indicating that the model is improving its fit to the data\n",
    "\n",
    "Training accuracy increased, showing that the model is learning more from the training data\n",
    "\n",
    "Validation accuracy also increased, indicating better generalization.\n",
    "\n",
    "The model acchieved highest training accuracy at epoch 5 but the validation loss increased from epoch 3 till epoch 5 and the valid accuracy decreased slightly in epoch 5\n",
    "\n",
    "That's why i made early stopping, i keep the modell running with no improvements in validation loss for 3 epochs incase it improves but it stopped to not cause overfitting\n",
    "\n",
    "The model achieved its best validation accuracy in epoch 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "\n",
    "    # Make the model evaluate\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the correct predictions, total samples, and total loss\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Initiatlize 2 lists of all the predicted and all the true values\n",
    "    all_predicted = []\n",
    "    all_targets = []\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "\n",
    "            # Create a binary mask: True values are non-padding tokens, and False values are padding tokens\n",
    "            # Ignore padding tokens when calculating loss\n",
    "            mask = (inputs != 0)\n",
    "\n",
    "            # Pass the model the input data to make predictions\n",
    "            outputs = model(inputs, mask)\n",
    "\n",
    "            # Calculate the loss between model predictions and actual labels\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Update the total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Find the class with the highest prediction for each sample in the batch\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            # Update the number of samples processed\n",
    "            total += targets.size(0)\n",
    "\n",
    "            # Count the correct predictions by comparing the predictions with the actual labels\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            # Append predicted and true labels for classification report\n",
    "            all_predicted.extend(predicted.tolist())\n",
    "            all_targets.extend(targets.tolist())\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Calculate the average loss\n",
    "    average_loss = total_loss / len(test_loader)\n",
    "\n",
    "    print(f\"Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Generate and print the classification report\n",
    "\n",
    "    report = classification_report(all_targets, all_predicted)\n",
    "    print(\"\\n\" + report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1910, Test Accuracy: 94.90%\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97     28249\n",
      "           1       0.76      0.74      0.75      3279\n",
      "\n",
      "    accuracy                           0.95     31528\n",
      "   macro avg       0.87      0.86      0.86     31528\n",
      "weighted avg       0.95      0.95      0.95     31528\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the test function\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows great performance with high test accuracy, indicating that it can correctly classify with 94.90% accuracy\n",
    "\n",
    "For class 0, the model has high precision, recall, and F1-score, suggesting it performs well in correctly classifying this class\n",
    "\n",
    "For class 1, the model's performance is slightly lower, with lower precision, recall, and F1-score values, indicating that it may have some difficulty in correctly classifying this class. But the F1-score (0.75) for this class suggests a reasonable balance between precision and recall\n",
    "\n",
    "The model over all shows that it's good for correctly classyfing classes 0 and 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
